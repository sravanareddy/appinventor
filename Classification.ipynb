{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import ujson\n",
    "import numpy \n",
    "data = ujson.load(open('user_project_times.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46\n"
     ]
    }
   ],
   "source": [
    "summaries = {}\n",
    "ctr = 0\n",
    "for line in open('user_project_summaries.json'): # lazy iteration because the file is large\n",
    "    print ctr,\n",
    "    ctr+=1\n",
    "    summaries.update(ujson.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Language Data Set and Initialize Language Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading keys\n",
      "lang data created\n"
     ]
    }
   ],
   "source": [
    "'''We are using all the data from the user from whom we we have previously \n",
    "classfied what their primary langauge is '''\n",
    "user_langs = ujson.load(open('user_inferredlangs.json')) \n",
    "lang_data = {}\n",
    "keys = user_langs.keys()\n",
    "\n",
    "#remove Latin \n",
    "# noLatin = []\n",
    "# for key in keys: \n",
    "#     if \"la\" != user_langs[key:\n",
    "#         noLatin.append(key)\n",
    "# keys = noLatin\n",
    "                          \n",
    "print 'done loading keys'\n",
    "for key in keys: \n",
    "    lang_data[key] = data[key[0:2] + '/' + key]\n",
    "print 'lang data created'\n",
    "\n",
    "lang_dict = {} #key is langauge isocode, value is number \n",
    "isocodes = ujson.load(open('isocodes.json'))\n",
    "counter = 0 \n",
    "for lang in isocodes:\n",
    "    lang_dict[lang] = counter\n",
    "    counter+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_time(el):\n",
    "    el = int(str(el)[:10])\n",
    "    return datetime.date.fromtimestamp(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest: 2013-03-27\n",
      "Latest: 2016-03-10\n",
      "Filtered to 25324 users from 46319\n"
     ]
    }
   ],
   "source": [
    "earliest = 1e100\n",
    "latest = 0\n",
    "for user in data:\n",
    "    for ctime, _ in data[user]: #ignore last-modified for now\n",
    "        if ctime < earliest:\n",
    "            earliest = ctime\n",
    "        if ctime > latest:\n",
    "            latest = ctime\n",
    "\n",
    "print 'Earliest:', convert_time(earliest) \n",
    "print 'Latest:', convert_time(latest)\n",
    "MAXDUR = 300\n",
    "MINDUR = 150\n",
    "# get subset of data from \"old\" users whose earliest creation date is more than MAXDUR days before the end of the dataset\n",
    "# (giving them a good chance to stay on for more than MINDUR days),\n",
    "# AND have activity within MAXDUR days if they are active beyond MINDUR (to be fair to recent users)\n",
    "old_data = {}\n",
    "for user in data:\n",
    "    start_time = min([ctime for ctime, _ in data[user]])\n",
    "    if int((latest-start_time)/(86400.*1000))>MAXDUR:\n",
    "        post = filter(lambda length: length>MINDUR, \n",
    "                      map(lambda (ctime,mtime): (ctime-start_time)/(86400.*1000), data[user]))\n",
    "        if len(post)>0 and len(filter(lambda length: length<=MAXDUR, post))==0:\n",
    "            continue  # ignore users who have no activity in the 120-180 period\n",
    "        old_data[user[3:]] = summaries[user[3:]]\n",
    "print 'Filtered to', len(old_data), 'users from', len(data)\n",
    "\n",
    "old_user_activity_length = numpy.zeros(len(old_data))\n",
    "for i, user in enumerate(old_data.keys()):\n",
    "    ctimes = [ctime for ctime, _ in data[user[0:2] + '/' + user]]\n",
    "    old_user_activity_length[i] = (max(ctimes) - min(ctimes))/(86400*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TASK = \"RETENTION\"\n",
    "if TASK == 'RETENTION': \n",
    "    data = old_data\n",
    "if TASK == 'LANGUAGE':\n",
    "    data = lang_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "import os \n",
    "from features import * \n",
    "sys.path.append(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getEarliest(userID): #helper function to get the earliest and latest project creation date for summmaries\n",
    "    user = summaries[userID]\n",
    "    earliest = user.keys()[0]\n",
    "    for proj in user: \n",
    "        creationDate = summaries[userID][proj]['**created']\n",
    "        if  creationDate < earliest: \n",
    "            earliest = creationDate\n",
    "    return earliest\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getProjectsInMINDUR(userID): #for summaries\n",
    "    projDict = {}\n",
    "    earliest = getEarliest(userID)\n",
    "\n",
    "    latest = (MINDUR * 86400 * 1000) + earliest \n",
    "    user = summaries[userID]\n",
    "    for proj in user: \n",
    "        if summaries[userID][proj]['**created'] >= earliest and summaries[userID][proj]['**created'] <= latest: \n",
    "            projDict[proj] = summaries[userID][proj]\n",
    "    return projDict\n",
    "                        \n",
    "                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData(userID):\n",
    "    if TASK == 'LANGUAGE': \n",
    "        return {userID: summaries[userID]} \n",
    "    if TASK == 'RETENTION':\n",
    "        return {userID : getProjectsInMINDUR(userID)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getProjects(userID): \n",
    "    r = getData(userID)\n",
    "    projects = r[r.keys()[0]] \n",
    "    #print projects[projects.keys()[0]]['**created']\n",
    "\n",
    "    name_time = [(project, projects[project]['**created']) for project in projects]\n",
    "    name_time = sorted(name_time, key=lambda t: t[1])  \n",
    "    proj_list = [summaries[userID][project[0]] for project in name_time]\n",
    "    return proj_list\n",
    "    #print proj_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def percentProjectsFeatures(projects):\n",
    "    \"\"\"return a dictionary mapping each time slot (10%, 20%, etc) to the percentage of projects created\n",
    "    at that time,\n",
    "    for a particular user represented as list of (creation times, modified times),\n",
    "    using only their first MINDUR days of activity\"\"\"   \n",
    "    userDict = {} \n",
    "    #note: feature names are for our reference only\n",
    "    userDict[\"average length\"] = getAverageProjectLen(projects)\n",
    "    userDict[\"num projects\"] = numProjects(projects)\n",
    "    \n",
    "    decileProjects = numProjectsInDecile(projects)\n",
    "    \n",
    "    userDict[\"1st\"] = decileProjects[0]\n",
    "    userDict[\"2nd\"] = decileProjects[1]\n",
    "    userDict[\"3rd\"] = decileProjects[2]\n",
    "    userDict[\"4th\"] = decileProjects[3]\n",
    "    userDict[\"5th\"] = decileProjects[4]\n",
    "    userDict[\"6th\"] = decileProjects[5]\n",
    "    userDict[\"7th\"] = decileProjects[6]\n",
    "    userDict[\"8th\"] = decileProjects[7]\n",
    "    userDict[\"9th\"] = decileProjects[8]\n",
    "    userDict[\"10th\"] = decileProjects[9]\n",
    "\n",
    "    return userDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dayAnalysisFeatures(projects): \n",
    "    userDict = {} \n",
    "    userDict[\"weekday\"] = percentOnWeekday(projects) #\n",
    "    day = percentOnDay(projects)\n",
    "    userDict[\"Monday\"] = day[0]\n",
    "    userDict[\"Tuesday\"] = day[1]\n",
    "    userDict[\"Wednesday\"] =day[2]\n",
    "    userDict[\"Thursday\"] = day[3]\n",
    "    userDict[\"Friday\"] = day[4]\n",
    "    userDict[\"Saturday\"] = day[5]\n",
    "    userDict[\"Sunday\"] = day[6]\n",
    "\n",
    "    return userDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summaryOBlockDecile(projects): \n",
    "    userDict = {}\n",
    "    decileOrphanBlock = decileOrphanBlocks(projects)\n",
    "    userDict[\"O 1\"] = decileOrphanBlock[0]\n",
    "    userDict[\"O 2\"] = decileOrphanBlock[1]\n",
    "    userDict[\"O 3\"] = decileOrphanBlock[2]\n",
    "    userDict[\"O 4\"] = decileOrphanBlock[3]\n",
    "    userDict[\"O 5\"] = decileOrphanBlock[4]\n",
    "    userDict[\"O 6\"] = decileOrphanBlock[5]\n",
    "    userDict[\"O 7\"] = decileOrphanBlock[6]\n",
    "    userDict[\"O 8\"] = decileOrphanBlock[7]\n",
    "    userDict[\"O 9\"] = decileOrphanBlock[8]\n",
    "    userDict[\"O 10\"] = decileOrphanBlock[9]\n",
    "    return userDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summaryDecileTLBlocks(projects): \n",
    "    userDict = {}\n",
    "    decileTypesTopLevelBlocks = decileNumTopLevelBlocks(projects)\n",
    "    userDict[\"TL 1\"] = decileNumTopLevelBlock[0]\n",
    "    userDict[\"TL 2\"] = decileNumTopLevelBlock[1]\n",
    "    userDict[\"TL 3\"] = decileNumTopLevelBlock[2]\n",
    "    userDict[\"TL 4\"] = decileNumTopLevelBlock[3]\n",
    "    userDict[\"TL 5\"] = decileNumTopLevelBlock[4]\n",
    "    userDict[\"TL 6\"] = decileNumTopLevelBlock[5]\n",
    "    userDict[\"TL 7\"] = decileNumTopLevelBlock[6]\n",
    "    userDict[\"TL 8\"] = decileNumTopLevelBlock[7]\n",
    "    userDict[\"TL 9\"] = decileNumTopLevelBlock[8]\n",
    "    userDict[\"TL 10\"] = decileNumTopLevelBlock[9]\n",
    "    return userDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summaryDecileNumScreens(projects): \n",
    "    userDict = {}\n",
    "    decileNumS = decileNumScreens(projects)\n",
    "    userDict[\"NS 1\"] = decileNumS[0]\n",
    "    userDict[\"NS 2\"] = decileNumS[1]\n",
    "    userDict[\"NS 3\"] = decileNumS[2]\n",
    "    userDict[\"NS 4\"] = decileNumS[3]\n",
    "    userDict[\"NS 5\"] = decileNumS[4]\n",
    "    userDict[\"NS 6\"] = decileNumS[5]\n",
    "    userDict[\"NS 7\"] = decileNumS[6]\n",
    "    userDict[\"NS 8\"] = decileNumS[7]\n",
    "    userDict[\"NS 9\"] = decileNumS[8]\n",
    "    userDict[\"NS 10\"] = decileNumS[9]\n",
    "    return userDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summaryAverages(projects):\n",
    "    userDict = {} \n",
    "    userDict[\"NS\"] = averageNumScreens(projects)\n",
    "    userDict[\"NB\"] = averageNumBlocks(projects)\n",
    "    userDict[\"OB\"] = getAverageOrphanBlocks(projects)\n",
    "    userDict[\"TL\"] = getAverageTypeTLBlocks(projects)\n",
    "    userDict[\"TL2\"] = getAverageNumTLBlocks(projects)\n",
    "    userDict[\"NC\"] = averageNumComponents(projects)\n",
    "    userDict[\"MC\"] = aveNumMediaAssets(projects)\n",
    "    varList = getAllVariables(projects)\n",
    "    \n",
    "    userDict[\"local vars\"] = varList[0]\n",
    "    userDict[\"global vars\"] = varList[1]\n",
    "    \n",
    "    return userDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write other feature functions -- perhaps different interval sizes (instead of 10%), proportion of projects rather than number, etc. With some effort, we can get a measure of activity on weekends/evenings as features too. Experiment...\n",
    "\n",
    "Give each feature function a distinct name, and re-run the above two cells with the function in place of `featurize` to see if the contingency tables change\n",
    "\n",
    "Remember from the CS111 assignment that we can combine a list of feature functions to give a new feature function, so you should experiment with combinations as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_featfuncs(funclist):\n",
    "    def combined(user):\n",
    "        basedict = funclist[0](user)\n",
    "        for f in funclist[1:]:\n",
    "            basedict.update(f(user))\n",
    "        return basedict\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RETENTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name __check_build",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-2ea50e5f1385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0muser_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# process, as it may not be compiled yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[0;31m# avoid flakes unused variable error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name __check_build"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "user_dicts = []\n",
    "all_features = []\n",
    "time_features = [] \n",
    "code_features = []\n",
    "\n",
    "# i = 0\n",
    "\n",
    "if TASK == \"RETENTION\":\n",
    "    i = 0\n",
    "    for user in data:\n",
    "        i +=1 \n",
    "        if i % 1000 == 0: \n",
    "            print i\n",
    "        projects = getProjects(user)\n",
    "        all_combined = combine_featfuncs([percentProjectsFeatures, dayAnalysisFeatures, summaryDecileNumScreens,summaryOBlockDecile,summaryDecileTLBlocks,summaryAverages])\n",
    "        time_combined = combine_featfuncs([percentProjectsFeatures, dayAnalysisFeatures])\n",
    "        code_combined = combine_featfuncs([summaryDecileNumScreens,summaryOBlockDecile,summaryDecileTLBlocks,summaryAverages]) \n",
    "    \n",
    "        all_features.append(all_combined(projects))\n",
    "        time_features.append(time_combined(projects))\n",
    "        code_features.append(code_combined(projects))\n",
    "\n",
    "\n",
    "    print ('done loading featurizers')\n",
    "    \n",
    "\n",
    "    vec = DictVectorizer()\n",
    "    X1 = vec.fit_transform(all_features) # converts to array: http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    X2 = vec.fit_transform(time_features)\n",
    "    X3 = vec.fit_transform(code_features)\n",
    "    #X = vec.fit_transform(user_dicts) # converts to array: http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "    #X = normalize(X, axis=0, norm='l1') #normalization is bringing the values down significantly \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 1: Clustering.** Is there any correlation between the clusters produced by these features, and whether or not a user is retained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name __check_build",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-2d4ee4b83501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTASK\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'RETENTION'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mKMeans_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0musers_clusterids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# process, as it may not be compiled yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[0;31m# avoid flakes unused variable error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name __check_build"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "if TASK == 'RETENTION':\n",
    "\n",
    "    KMeans_model = KMeans(n_clusters=2)\n",
    "    users_clusterids = KMeans_model.fit_predict(X1)\n",
    "\n",
    "    from scipy.stats import chi2_contingency\n",
    "    conting = numpy.zeros((2, 2))  # row = cluster ID (0, 1), col = #days (0 for <=120, 1 for >120)\n",
    "\n",
    "    for i in range(len(users_clusterids)):\n",
    "        cluster = users_clusterids[i]\n",
    "        if old_user_activity_length[i]<=MINDUR:\n",
    "            conting[cluster, 0] += 1\n",
    "        else:\n",
    "            conting[cluster, 1] += 1\n",
    "\n",
    "# visualize\n",
    "    print conting\n",
    "\n",
    "# compute chi2 statistic to get the probability that there is no association between\n",
    "# the clustering and whether or not the user is active for >120 days\n",
    "# lower probabilities are better\n",
    "    _, p, _, _ = chi2_contingency(conting)\n",
    "    print 'There is a', p, 'probability that the clusters and retention are independent.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 2: Classification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from  sklearn.metrics import f1_score \n",
    "from sklearn import linear_model\n",
    "\n",
    "if TASK=='RETENTION':\n",
    "    y = numpy.array(map(lambda d: int(d>MINDUR), old_user_activity_length))  # labels (0 for less than MINDUR, 1 otherwise)\n",
    "    #X1 = normalize(X1, axis=0, norm='l1')\n",
    "    X2 = normalize(X2, axis=0, norm='l1')\n",
    "    percentpos = sum(y)/float(len(y))\n",
    "    chance = max(percentpos, 1-percentpos)\n",
    "    print 'Chance is', chance\n",
    "\n",
    "    print 'Building a model with', X1.shape[1], 'features for', TASK\n",
    "    all_coefs = [] \n",
    "    for trainidx, testidx in StratifiedKFold(y):\n",
    "        print 'Fold'\n",
    "        k = 70 #num neighbors\n",
    "        ytrain = y[trainidx]\n",
    "        ytest = y[testidx]\n",
    "        \n",
    "        X1train = X1[trainidx, :]  # using numpy's smart indexing\n",
    "        X1test = X1[testidx, :]\n",
    "        \n",
    "        X2train = X2[trainidx, :]  # using numpy's smart indexing\n",
    "        X2test = X2[testidx, :]\n",
    "    \n",
    "        X3train = X3[trainidx, :]  # using numpy's smart indexing\n",
    "        X3test = X3[testidx, :]\n",
    "    \n",
    "        KNeighbors_model1 = KNeighborsClassifier(n_neighbors=k)\n",
    "        logistic_model1 = linear_model.LogisticRegression()\n",
    "\n",
    "        KNeighbors_model2 = KNeighborsClassifier(n_neighbors=k)\n",
    "        logistic_model2 = linear_model.LogisticRegression()\n",
    "\n",
    "        KNeighbors_model3 = KNeighborsClassifier(n_neighbors=k)  \n",
    "        logistic_model3 = linear_model.LogisticRegression()\n",
    "\n",
    "        KNeighbors_model1.fit(X1train, ytrain)\n",
    "        logistic_model1.fit(X1train, ytrain)\n",
    "\n",
    "        KNN_XtestP1 = KNeighbors_model1.predict(X1test)\n",
    "        log_XtestP1 = logistic_model1.predict(X1test)\n",
    "    \n",
    "        KNeighbors_model2.fit(X2train, ytrain)\n",
    "        logistic_model2.fit(X2train, ytrain)\n",
    "\n",
    "        KNN_XtestP2 = KNeighbors_model2.predict(X2test)\n",
    "        log_XtestP2 = logistic_model2.predict(X2test)\n",
    "        \n",
    "        KNeighbors_model3.fit(X3train, ytrain)\n",
    "        logistic_model3.fit(X3train, ytrain)\n",
    "\n",
    "        KNN_XtestP3 = KNeighbors_model3.predict(X3test)\n",
    "        log_XtestP3 = logistic_model3.predict(X3test)\n",
    "    \n",
    "    \n",
    "    \n",
    "        print k, 'neighbors' \n",
    "        print \"All features KNN Score: \", KNeighbors_model1.score(X1test, ytest)\n",
    "        print 'All features KNN f1 score', f1_score(KNN_XtestP1, ytest),'\\n'\n",
    "        \n",
    "        print'All features Logistic Regression Score: ', logistic_model1.score(X1test, ytest)\n",
    "        print 'All features Logistic f1 score', f1_score(log_XtestP1, ytest) \n",
    "        print \"Number predicted who survived\", float(sum(log_XtestP1))/ len(log_XtestP1)\n",
    "\n",
    "\n",
    "        print \"Time features KNN Score: \", KNeighbors_model2.score(X2test, ytest)\n",
    "        print 'Time features KNN f1 score', f1_score(KNN_XtestP2, ytest),'\\n'\n",
    "\n",
    "        print'Time features Logistic Regression Score: ', logistic_model2.score(X2test, ytest)\n",
    "        print 'Time features Logistic f1 score', f1_score(log_XtestP2, ytest) \n",
    "        \n",
    "        print \"Code features KNN Score: \", KNeighbors_model3.score(X3test, ytest)\n",
    "        print 'Code features KNN f1 score', f1_score(KNN_XtestP3, ytest),'\\n'\n",
    "\n",
    "        print'Code features Logistic Regression Score: ', logistic_model3.score(X3test, ytest)\n",
    "        print 'Code features Logistic f1 score', f1_score(log_XtestP3, ytest) \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "LANGUAGE:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from features import *\n",
    "all_features = []\n",
    "time_features = [] \n",
    "code_features = []\n",
    "if TASK == \"LANGUAGE\": \n",
    "    users = lang_data.keys()\n",
    "    print ('done loading keys')\n",
    "    i = 0 \n",
    "    for user in users:\n",
    "        i +=1 \n",
    "        if i % 1000 == 0: \n",
    "            print i\n",
    "        projects = getProjects(user)\n",
    "        all_combined = combine_featfuncs([percentProjectsFeatures, dayAnalysisFeatures, summaryDecileNumScreens,summaryOBlockDecile,summaryDecileTLBlocks,summaryAverages])\n",
    "        time_combined = combine_featfuncs([percentProjectsFeatures, dayAnalysisFeatures])\n",
    "        code_combined = combine_featfuncs([summaryDecileNumScreens,summaryOBlockDecile,summaryDecileTLBlocks,summaryAverages]) \n",
    "\n",
    "        all_features.append(all_combined(projects))\n",
    "        time_features.append(time_combined(projects))\n",
    "        code_features.append(code_combined(projects))\n",
    "\n",
    "    # for userID in lang_data: #user is just the user's id num \n",
    "    #     combined = combine_featfuncs([percentProjectsFeatures, dayAnalysisFeatures, decileFeatures ,summaryNumScreensDecile,summaryOBlockDecile,summaryDecileTLBlocks,summaryAverages]) #a function object is being returned here?? \n",
    "    #     user_dicts.append(combined(userID))\n",
    "\n",
    "\n",
    "    print ('done loading featurizers')\n",
    "\n",
    "\n",
    "    vec = DictVectorizer()\n",
    "    X1 = vec.fit_transform(all_features) # converts to array: http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    X2 = vec.fit_transform(time_features)\n",
    "    X3 = vec.fit_transform(code_features)\n",
    "    #X = normalize(X, axis=0, norm='l1') #normalization is bringing the values down significantly \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print len(data)\n",
    "# print len(lang_data)\n",
    "# #what y is\n",
    "# print [lang_dict[user_langs[userID]] for userID in lang_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from  sklearn.metrics import f1_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "print old_user_activity_length[0:100]\n",
    "if TASK=='LANGUAGE':\n",
    "    y =  numpy.array([lang_dict[user_langs[userID]] for userID in lang_data]) # labels\n",
    "\n",
    "    percentpos = sum(y)/float(len(y))\n",
    "    chance = max(percentpos, 1-percentpos)\n",
    "    print 'Chance is', chance\n",
    "\n",
    "    #print 'Building a model with', X1.shape[1], 'features for', TASK\n",
    "    all_coefs = [] \n",
    "    for trainidx, testidx in StratifiedKFold(y):\n",
    "        print 'Fold'\n",
    "        k = 70 #num neighbors\n",
    "        X1train = X1[trainidx, :]  # using numpy's smart indexing\n",
    "        X1test = X1[testidx, :]\n",
    "        ytrain = y[trainidx]\n",
    "        ytest = y[testidx]\n",
    "\n",
    "        X2train = X2[trainidx, :]  # using numpy's smart indexing\n",
    "        X2test = X2[testidx, :]\n",
    "\n",
    "        X3train = X3[trainidx, :]  # using numpy's smart indexing\n",
    "        X3test = X3[testidx, :]\n",
    "\n",
    "        KNeighbors_model1 = KNeighborsClassifier(n_neighbors=k)  \n",
    "        KNeighbors_model2 = KNeighborsClassifier(n_neighbors=k)  \n",
    "        KNeighbors_model3 = KNeighborsClassifier(n_neighbors=k)  \n",
    "\n",
    "        print('models created')\n",
    "\n",
    "        KNeighbors_model1.fit(X1train, ytrain)\n",
    "        print 'model1 fit'\n",
    "        KNeighbors_model2.fit(X2train, ytrain)\n",
    "        print 'model2 fit'\n",
    "        KNeighbors_model3.fit(X3train, ytrain)  \n",
    "        print ('all models fit')\n",
    "\n",
    "        KNN_XtestP1 = KNeighbors_model1.predict(X1test)\n",
    "        print 'model1 predicted'\n",
    "        KNN_XtestP2 = KNeighbors_model2.predict(X2test)\n",
    "        print 'model2 predicted'\n",
    "        KNN_XtestP3 = KNeighbors_model3.predict(X3test)\n",
    "        print 'models predicted \\n'\n",
    "\n",
    "        print k, 'neighbors' \n",
    "        print \"All features KNN Score: \", KNeighbors_model1.score(X1test, ytest)\n",
    "        print 'All features KNN f1 score', f1_score(KNN_XtestP1, ytest),'\\n'\n",
    "\n",
    "        print \"time features KNN Score: \", KNeighbors_model2.score(X2test, ytest)\n",
    "        print 'time features KNN f1 score', f1_score(KNN_XtestP2, ytest),'\\n'\n",
    "\n",
    "        print \"code features KNN Score: \", KNeighbors_model3.score(X3test, ytest)\n",
    "        print 'code features KNN f1 score', f1_score(KNN_XtestP3, ytest),'\\n'\n",
    "\n",
    "    cnf_matrix = confusion_matrix(ytest, KNN_XtestP1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnf_matrix = list(cnf_matrix)\n",
    "print len(cnf_matrix)\n",
    "print len(lang_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [ipykernel_py2]",
   "language": "python",
   "name": "Python [ipykernel_py2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
