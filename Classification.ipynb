{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import ujson\n",
    "import numpy \n",
    "data = ujson.load(open('user_project_times.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46\n"
     ]
    }
   ],
   "source": [
    "summaries = {}\n",
    "ctr = 0\n",
    "for line in open('user_project_summaries.json'): # lazy iteration because the file is large\n",
    "    print ctr,\n",
    "    ctr+=1\n",
    "    summaries.update(ujson.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TASK = \"RETENTION\"\n",
    "if TASK == 'RETENTION': \n",
    "    data = old_data\n",
    "if TASK == 'LANGUAGE':\n",
    "    data = lang_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Language Data Set and Initialize Language Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading keys\n",
      "lang data created\n"
     ]
    }
   ],
   "source": [
    "'''We are using all the data from the user from whom we we have previously \n",
    "classfied what their primary langauge is '''\n",
    "user_langs = ujson.load(open('user_inferredlangs.json')) \n",
    "lang_data = {}\n",
    "keys = user_langs.keys()\n",
    "\n",
    "print 'done loading keys'\n",
    "for key in keys: \n",
    "    lang_data[key] = data[key[0:2] + '/' + key]\n",
    "print 'lang data created'\n",
    "\n",
    "lang_dict = {} #key is langauge isocode, value is number \n",
    "isocodes = ujson.load(open('isocodes.json'))\n",
    "counter = 0 \n",
    "for lang in isocodes:\n",
    "    lang_dict[lang] = counter\n",
    "    counter+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_time(el):\n",
    "    el = int(str(el)[:10])\n",
    "    return datetime.date.fromtimestamp(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the length of activity for each user (still using creation times) in days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "user_activity_length = numpy.zeros(len(data))\n",
    "for i, user in enumerate(data.keys()):\n",
    "    ctimes = [ctime for ctime, _ in data[user]]\n",
    "    user_activity_length[i] = (max(ctimes) - min(ctimes))/(86400*1000)\n",
    "print len(user_activity_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows the number of users whose activity time was within `x` number of days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest: 2013-03-27\n",
      "Latest: 2016-03-10\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "u'42'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-3534949dc38e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mMAXDUR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# ignore users who have no activity in the 120-180 period\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mold_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Filtered to'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'users from'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: u'42'"
     ]
    }
   ],
   "source": [
    "earliest = 1e100\n",
    "latest = 0\n",
    "for user in data:\n",
    "    for ctime, _ in data[user]: #ignore last-modified for now\n",
    "        if ctime < earliest:\n",
    "            earliest = ctime\n",
    "        if ctime > latest:\n",
    "            latest = ctime\n",
    "\n",
    "print 'Earliest:', convert_time(earliest) \n",
    "print 'Latest:', convert_time(latest)\n",
    "MAXDUR = 300\n",
    "MINDUR = 150\n",
    "# get subset of data from \"old\" users whose earliest creation date is more than MAXDUR days before the end of the dataset\n",
    "# (giving them a good chance to stay on for more than MINDUR days),\n",
    "# AND have activity within MAXDUR days if they are active beyond MINDUR (to be fair to recent users)\n",
    "old_data = {}\n",
    "for user in data:\n",
    "    start_time = min([ctime for ctime, _ in data[user]])\n",
    "    if int((latest-start_time)/(86400.*1000))>MAXDUR:\n",
    "        post = filter(lambda length: length>MINDUR, \n",
    "                      map(lambda (ctime,mtime): (ctime-start_time)/(86400.*1000), data[user]))\n",
    "        if len(post)>0 and len(filter(lambda length: length<=MAXDUR, post))==0:\n",
    "            continue  # ignore users who have no activity in the 120-180 period\n",
    "        old_data[user[3:]] = summaries[user[3:]]\n",
    "print 'Filtered to', len(old_data), 'users from', len(data)\n",
    "\n",
    "old_user_activity_length = numpy.zeros(len(old_data))\n",
    "for i, user in enumerate(old_data.keys()):\n",
    "    ctimes = [ctime for ctime, _ in data[user[0:2] + '/' + user]]\n",
    "    old_user_activity_length[i] = (max(ctimes) - min(ctimes))/(86400*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getEarliest(userID): #helper function to get the earliest and latest project creation date for summmaries\n",
    "    user = summaries[userID]\n",
    "    earliest = user.keys()[0]\n",
    "    for proj in user: \n",
    "        creationDate = summaries[userID][proj]['**created']\n",
    "        if  creationDate < earliest: \n",
    "            earliest = creationDate\n",
    "    return earliest\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getProjectsInMINDUR(userID): #for summaries\n",
    "    projDict = {}\n",
    "    earliest = getEarliest(userID)\n",
    "\n",
    "    latest = (MINDUR * 86400 * 1000) + earliest \n",
    "    user = summaries[userID]\n",
    "    for proj in user: \n",
    "        if summaries[userID][proj]['**created'] >= earliest and summaries[userID][proj]['**created'] <= latest: \n",
    "            projDict[proj] = summaries[userID][proj]\n",
    "    return projDict\n",
    "                        \n",
    "                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData(userID):\n",
    "    if TASK == 'LANGUAGE': \n",
    "        return {userID: summaries[userID]} \n",
    "    if TASK == 'RETENTION':\n",
    "        return {userID : getProjectsInMINDUR(userID)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getData('11542')\n",
    "# #summaries['11542'][u'p020_020_p0930d'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getProjects(userID): \n",
    "    r = getData(userID)\n",
    "    projects = r[r.keys()[0]] \n",
    "    #print projects[projects.keys()[0]]['**created']\n",
    "\n",
    "    name_time = [(project, projects[project]['**created']) for project in projects]\n",
    "    name_time = sorted(name_time, key=lambda t: t[1])  \n",
    "    proj_list = [summaries[userID][project[0]] for project in name_time]\n",
    "    return proj_list\n",
    "    #print proj_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAverageProjectLen(projects):\n",
    "    num_projects = len(projects)\n",
    "    total_project_length = 0 #the sum of all the lengths of the project (from date created to date modified)\n",
    "    \n",
    "    prev = projects[0][\"**created\"]\n",
    "    for project in projects[1:]: \n",
    "        curr = project['**created']\n",
    "        total_project_length += (curr - prev)/(86400.*1000)   #SR: added this to scale to days\n",
    "        prev = curr \n",
    "    return int(total_project_length / float(num_projects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numProjects(projects):\n",
    "    return len(projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percentOnWeekday(projects):\n",
    "    num_projs = numProjects(projects)\n",
    "    weekday_projs = 0\n",
    "    for project in projects: \n",
    "        if datetime.datetime.fromtimestamp(project['**created'] / 1e3).weekday() < 6: \n",
    "            weekday_projs += 1.0\n",
    "    return weekday_projs / num_projs * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def percentOnDay(projects):\n",
    "    num_projs = len(projects)\n",
    "    weekday_projs = 0\n",
    "    dow = [] \n",
    "    for i in range(7):\n",
    "        dow.append(0)\n",
    "    for project in projects: \n",
    "        date = datetime.datetime.fromtimestamp(project['**created'] / 1e3).weekday() \n",
    "        dow[date] += 1.0\n",
    "    for i in range(7):\n",
    "        dow[i] = dow[i] / num_projs * 100 \n",
    "    return dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " def numProjectsInDecile(projects):\n",
    "    ''' the first decile is the 0th decile (covering from 0-10% of the time), \n",
    "    the last decile is the 9th decile'''\n",
    "    #still not working right\n",
    "    first= int(projects[0]['**created'])\n",
    "    #print first\n",
    "    last = int(projects[len(projects)-1]['**created'])\n",
    "    decile_width = ((last - earliest) / (86400 * 1000)) / 10\n",
    "    if decile_width < 1: decile_width = 1\n",
    "    #print last\n",
    "    #print decile_width\n",
    "    decileList = []\n",
    "    \n",
    "    for i in range(10): \n",
    "        decileList.append(0)\n",
    "        \n",
    "    for project in projects: \n",
    "        if project['**created'] >= first and project['**created'] <= last: \n",
    "            index  = int(((project['**created']-first)/(86400 * 1000)) / (decile_width / 10.))\n",
    "            #print index\n",
    "            if index >= 10: \n",
    "                decileList[9] = decileList[9] + 1\n",
    "            else: \n",
    "                decileList[index] = decileList[index] + 1\n",
    "    return decileList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDecileBoundaries(projects):\n",
    "    first= projects[len(projects)-1]['**created']\n",
    "    decile_width = MINDUR * 86400000/ 10 \n",
    "    decileNums = []\n",
    "    for i in range(1,11): \n",
    "        decileNums.append(first + decile_width*i)\n",
    "    return decileNums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# users = old_data.keys()[0:10]\n",
    "# for userID in users:\n",
    "#     r = getProjects(userID)\n",
    "#     numProjectsInDecile(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Summary features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO ORPHAN BLOCKS\n"
     ]
    }
   ],
   "source": [
    "# print summaries['44884']['p021_020_clock']['Screen1']['Blocks'][\"Orphan Blocks\"]#.keys()#['*Top Level Blocks'].keys()#['Active Blocks'].keys()#['Local Variable Names'].keys() #['Local Variable Names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNumScreens(projects):\n",
    "    numScreens = []\n",
    "    for project in projects: \n",
    "         numScreens.append(project['*Number of Screens'])\n",
    "    return numScreens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def averageNumScreens(projects):\n",
    "    screenList = getNumScreens(projects)\n",
    "    return sum(screenList)/ len(screenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAveNumLocalVariables(projects):\n",
    "    numScreens = getNumScreens(projects)\n",
    "    lv_count = 0 \n",
    "    i = 0 \n",
    "    while i < len(projects): \n",
    "        for j in range(numScreens[i]):\n",
    "            screenNum = \"Screen\"+ str(j + 1)\n",
    "            if screenNum in projects[i].keys():\n",
    "                if  not isinstance(projects[i][screenNum]['Blocks'], unicode):\n",
    "                    if projects[i][screenNum]['Blocks']['Active Blocks'] != 'NO ACTIVE BLOCKS':\n",
    "                        lv_count += len(projects[i][screenNum]['Blocks']['Active Blocks']['Local Variable Names'].keys())\n",
    "        i+=1\n",
    "    return lv_count / float(len(projects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAveNumGlobalVariables(projects):\n",
    "    numScreens = getNumScreens(projects)\n",
    "    gv_count = 0 \n",
    "    i = 0 \n",
    "    while i < len(projects): \n",
    "        for j in range(numScreens[i]):\n",
    "            screenNum = \"Screen\"+ str(j + 1)\n",
    "            if screenNum in projects[i].keys():\n",
    "                if  not isinstance(projects[i][screenNum]['Blocks'], unicode):\n",
    "                    if projects[i][screenNum]['Blocks']['Active Blocks'] != 'NO ACTIVE BLOCKS':\n",
    "                        gv_count += len(projects[i][screenNum]['Blocks']['Active Blocks']['Global Variable Names'].keys())\n",
    "        i+=1\n",
    "    return gv_count / float(len(projects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllVariables(projects): \n",
    "    '''returns a list with 0th element being the average number of local variables, the 1st element\n",
    "    being the average number of global variables, and the 2nd element being the average number\n",
    "    of variables in a project'''\n",
    "    numScreens = getNumScreens(projects)\n",
    "    gv_count = 0 \n",
    "    lv_count = 0 \n",
    "    i = 0 \n",
    "    variables = []\n",
    "    while i < len(projects): \n",
    "        for j in range(numScreens[i]):\n",
    "            screenNum = \"Screen\"+ str(j + 1)\n",
    "            if screenNum in projects[i].keys():\n",
    "                if  not isinstance(projects[i][screenNum]['Blocks'], unicode):\n",
    "                    if projects[i][screenNum]['Blocks']['Active Blocks'] != 'NO ACTIVE BLOCKS':\n",
    "                        lv_count += len(projects[i][screenNum]['Blocks']['Active Blocks']['Local Variable Names'].keys())\n",
    "                        gv_count += len(projects[i][screenNum]['Blocks']['Active Blocks']['Global Variable Names'].keys())\n",
    "        i+=1\n",
    "    variables.append(lv_count)\n",
    "    variables.append(gv_count)\n",
    "    variables.append(lv_count + gv_count)\n",
    "    \n",
    "    return [v/float(len(projects)) for v in variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def averageNumBlocks(projects):\n",
    "    numScreens = getNumScreens(projects)\n",
    "    totalBlocks = 0 \n",
    "    i = 0 \n",
    "    while i < len(projects): \n",
    "        for j in range(numScreens[i]):\n",
    "            screenNum = \"Screen\"+ str(j + 1)\n",
    "            if screenNum in projects[i].keys():\n",
    "                if  not isinstance(projects[i][screenNum]['Blocks'], unicode):\n",
    "                    if  not isinstance(projects[i][screenNum]['Blocks']['Active Blocks'],unicode):\n",
    "                        totalBlocks += projects[i][screenNum]['Blocks']['Active Blocks']['*Number of Blocks']\n",
    "        i+=1\n",
    "    return totalBlocks / float(len(projects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAverageTLBlocks(projects):\n",
    "    numScreens = getNumScreens(projects)\n",
    "    tl_count = 0 \n",
    "    i = 0 \n",
    "    while i < len(projects): \n",
    "        for j in range(numScreens[i]):\n",
    "            screenNum = \"Screen\"+ str(j + 1)\n",
    "            if screenNum in projects[i].keys():\n",
    "                if  not isinstance(projects[i][screenNum]['Blocks'], unicode):\n",
    "                        tl_count += len(projects[i][screenNum]['Blocks']['*Top Level Blocks'].keys())\n",
    "        i+=1\n",
    "    return tl_count / float(len(projects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# users = lang_data.keys()[0:10000]\n",
    "# print 'loaded keys'\n",
    "# i = 0 \n",
    "# for userID in users:\n",
    "#     if i % 1000== 0: \n",
    "#         print i \n",
    "#     r = getProjects(userID)\n",
    "#     getAverageOrphanBlocks(r)\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAverageOrphanBlocks(projects):\n",
    "    numScreens = getNumScreens(projects)\n",
    "    tl_count = 0 \n",
    "    i = 0 \n",
    "    while i < len(projects): \n",
    "        for j in range(numScreens[i]):\n",
    "            screenNum = \"Screen\"+ str(j + 1)\n",
    "            if screenNum in projects[i].keys():\n",
    "                if  not isinstance(projects[i][screenNum]['Blocks'], unicode):\n",
    "                    if projects[i][screenNum]['Blocks']['Orphan Blocks'] != \"NO ORPHAN BLOCKS\": \n",
    "                        tl_count += projects[i][screenNum]['Blocks']['Orphan Blocks']['*Number of Blocks']\n",
    "        i+=1\n",
    "    return tl_count / float(len(projects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getDecileBoundaries(projects):\n",
    "    earliest = projects[0][\"**created\"]\n",
    "    #print (earliest + MINDUR * 86400000)\n",
    "\n",
    "    decile_width = (latest - earliest)/ 10 \n",
    "    #print decile_width\n",
    "    decileNums = []\n",
    "    for i in range(1,10): \n",
    "        decileNums.append(earliest + decile_width*i)\n",
    "    decileNums.append(projects[len(projects)-1]['**created'])\n",
    "    return decileNums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decileNumScreens(projects): \n",
    "    all_deciles = []\n",
    "    for i in range(10):\n",
    "         all_deciles.append([])\n",
    "    decile_boundaries = getDecileBoundaries(projects)\n",
    "    \n",
    "    for project in projects: \n",
    "        date = project['**created']\n",
    "        placed = False\n",
    "        for boundary in decile_boundaries: \n",
    "            if date < boundary and not placed: \n",
    "                index = decile_boundaries.index(boundary)\n",
    "                numScreens = project['*Number of Screens']\n",
    "                all_deciles[index].insert(0,numScreens)\n",
    "                placed = True\n",
    "    for i in range(10):\n",
    "        if len(all_deciles[i]) == 0: \n",
    "            all_deciles[i] = 0\n",
    "        else: \n",
    "            all_deciles[i] = sum(all_deciles[i])/ float(len(all_deciles[i]))\n",
    "    return all_deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decileNumTopLevelBlocks(projects): \n",
    "    '''measures how many TLblocks per project in each decile '''\n",
    "    all_deciles = []\n",
    "    for i in range(10):\n",
    "         all_deciles.append([])\n",
    "    decile_boundaries = getDecileBoundaries(projects)\n",
    "    \n",
    "    numScreens = getNumScreens(projects)\n",
    "    for i in range(len(projects)): \n",
    "        numTL =0 \n",
    "        for j in range(numScreens[i]):\n",
    "            screenNum = \"Screen\"+ str(j + 1)\n",
    "            if screenNum in projects[i].keys():\n",
    "                if  not isinstance(projects[i][screenNum]['Blocks'], unicode):\n",
    "                    numTL = len(projects[i][screenNum]['Blocks']['*Top Level Blocks'].keys())\n",
    "        date = projects[i]['**created']\n",
    "        placed = False\n",
    "        for boundary in decile_boundaries: \n",
    "            if date < boundary and not placed: \n",
    "                index = decile_boundaries.index(boundary)\n",
    "                all_deciles[index].insert(0,numTL)\n",
    "                placed = True\n",
    "    for i in range(10):\n",
    "        if len(all_deciles[i]) == 0: \n",
    "            all_deciles[i] = 0\n",
    "        else: \n",
    "            all_deciles[i] = sum(all_deciles[i])/ float(len(all_deciles[i]))\n",
    "    return all_deciles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decileOrphanBlocks(projects): \n",
    "    '''measures how many orphan blocks per project in each decile '''\n",
    "    all_deciles = []\n",
    "    for i in range(10):\n",
    "         all_deciles.append([])\n",
    "    decile_boundaries = getDecileBoundaries(projects)\n",
    "    \n",
    "    numScreens = getNumScreens(projects)\n",
    "    for i in range(len(projects)): \n",
    "        numOrphan =0 \n",
    "        for j in range(numScreens[i]):\n",
    "            screenNum = \"Screen\"+ str(j + 1)\n",
    "            if screenNum in projects[i].keys():\n",
    "                if  not isinstance(projects[i][screenNum]['Blocks'], unicode):\n",
    "                    if projects[i][screenNum]['Blocks']['Orphan Blocks'] != \"NO ORPHAN BLOCKS\":\n",
    "                        numOrphan += len(projects[i][screenNum]['Blocks']['Orphan Blocks'].keys())\n",
    "        date = projects[i]['**created']\n",
    "        placed = False\n",
    "        for boundary in decile_boundaries: \n",
    "            if date < boundary and not placed: \n",
    "                index = decile_boundaries.index(boundary)\n",
    "                all_deciles[index].insert(0,numOrphan)\n",
    "                placed = True\n",
    "    for i in range(10):\n",
    "        if len(all_deciles[i]) == 0: \n",
    "            all_deciles[i] = 0\n",
    "        else: \n",
    "            all_deciles[i] = sum(all_deciles[i])/ float(len(all_deciles[i]))\n",
    "    return all_deciles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# users = lang_data.keys()[0:10000]\n",
    "# print 'loaded keys'\n",
    "# i = 0 \n",
    "# for userID in users:\n",
    "#     if i % 1000== 0: \n",
    "#         print i \n",
    "#     r = getProjects(userID)\n",
    "#     decileOrphanBlocks(r)\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def decileNumScreens2(projects): \n",
    "#     all_deciles = []\n",
    "#     earliest = projects[0]['**created']\n",
    "#     latest = projects[len(projects)-1]['**created']\n",
    "#     decile_width = ((latest - earliest)/ (86400 * 1000.)) / 10\n",
    "\n",
    "#     for i in range(10):\n",
    "#          all_deciles.append([])\n",
    "            \n",
    "#     decile_boundaries = getDecileBoundaries(projects)\n",
    "#     for project in projects: \n",
    "#         date = project['**created']\n",
    "#         index  = (date-earliest)/(decile_width / 10)\n",
    "#         print index\n",
    "#         #all_deciles[index].insert(0, project['*Number of Screens'])\n",
    "#     for i in range(10):\n",
    "#         if len(all_deciles[i]) == 0: \n",
    "#             all_deciles[i] = 0\n",
    "#         else: \n",
    "#             all_deciles[i] = sum(all_deciles[i])/ float(len(all_deciles[i]))\n",
    "#     return all_deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def decileNumTopLevelBlocks2(userID): \n",
    "#     '''measures how many TLblocks per project in each decile '''\n",
    "#     all_deciles = []\n",
    "#     for i in range(10):\n",
    "#          all_deciles.append([])\n",
    "#     projs = getProjectsNamesInMINDUR(userID)\n",
    "#     decile_boundaries = getDecileBoundaries(userID)\n",
    "    \n",
    "#     for proj in projs: \n",
    "#         totalTL =0 \n",
    "#         numScreens = getNumScreens(userID, proj)\n",
    "#         for i in range(numScreens):\n",
    "#             screenNum = \"Screen\"+ str(i + 1)\n",
    "#             if screenNum in summaries[userID][proj].keys() and '*Top Level Blocks' in summaries[userID][proj][screenNum]['Blocks']: \n",
    "#                 typesOfTL = summaries[userID][proj][screenNum]['Blocks']['*Top Level Blocks'].keys()\n",
    "#                 for block in typesOfTL: \n",
    "#                     totalTL += summaries[userID][proj][screenNum]['Blocks']['*Top Level Blocks'][block]\n",
    "#         date = summaries[userID][proj]['**created']\n",
    "#         index  = ((date-earliest)/(86400 * 10000))/(MINDUR/10)\n",
    "#         #print index\n",
    "#         all_deciles[index].append(totalTL)\n",
    "#     for i in range(10):\n",
    "#         if len(all_deciles[i]) == 0: \n",
    "#             all_deciles[i] = 0\n",
    "#         else: \n",
    "#             all_deciles[i] = sum(all_deciles[i])/ float(len(all_deciles[i]))\n",
    "#     return all_deciles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# users = lang_data.keys()[0:1]\n",
    "# print 'loaded keys'\n",
    "# i = 0 \n",
    "# for userID in users:\n",
    "#     if i % 1000== 0: \n",
    "#         print i \n",
    "#     r = getProjects(userID)\n",
    "#     print decileNumScreens(r)\n",
    "#     print decileNumScreens2(r)\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def decileOrphanBlocks2(userID): \n",
    "#     '''measures how many orphan blocks per project in each decile '''\n",
    "#     all_deciles = []\n",
    "#     for i in range(10):\n",
    "#          all_deciles.append([])\n",
    "#     projs = getProjectsNamesInMINDUR(userID)\n",
    "#     decile_boundaries = getDecileBoundaries(userID)\n",
    "    \n",
    "#     for proj in projs: \n",
    "#         oBlocks =0 \n",
    "#         numScreens = getNumScreens(userID, proj)\n",
    "#         for i in range(numScreens):\n",
    "#             screenNum = \"Screen\"+ str(i + 1)\n",
    "#             if screenNum in summaries[userID][proj].keys() and 'Blocks' in summaries[userID][proj][screenNum].keys() and 'Orphan Blocks' in summaries[userID][proj][screenNum]['Blocks']: #occassional error otherwise\n",
    "#                 if summaries[userID][proj][screenNum]['Blocks']['Orphan Blocks'] != \"NO ORPHAN BLOCKS\":\n",
    "#                     try: \n",
    "#                         oBlocks += summaries[userID][proj][screenNum]['Blocks']['Orphan Blocks']['*Number of Blocks']\n",
    "#                     except TypeError: \n",
    "#                         print summaries[userID][proj][screenNum]['Blocks']['Orphan Blocks']['*Number of Blocks']\n",
    "#         date = summaries[userID][proj]['**created']\n",
    "#         index  = ((date-earliest)/(86400 * 10000))/(MINDUR/10)\n",
    "#         #print index\n",
    "#         all_deciles[index].insert(0,oBlocks)\n",
    "#     for i in range(10):\n",
    "#         if len(all_deciles[i]) == 0: \n",
    "#             all_deciles[i] = 0\n",
    "#         else: \n",
    "#             all_deciles[i] = sum(all_deciles[i])/ float(len(all_deciles[i]))\n",
    "#     return all_deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keys = summaries.keys()[0:100]\n",
    "# for userID in keys: \n",
    "#     print decileOrphanBlocks(userID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def percentProjectsFeatures(projects):\n",
    "    \"\"\"return a dictionary mapping each time slot (10%, 20%, etc) to the percentage of projects created\n",
    "    at that time,\n",
    "    for a particular user represented as list of (creation times, modified times),\n",
    "    using only their first MINDUR days of activity\"\"\"   \n",
    "    userDict = {} \n",
    "    #note: feature names are for our reference only\n",
    "    userDict[\"average length\"] = getAverageProjectLen(projects)\n",
    "    userDict[\"num projects\"] = numProjects(projects)\n",
    "    \n",
    "    decileProjects = numProjectsInDecile(projects)\n",
    "    \n",
    "    userDict[\"1st\"] = decileProjects[0]\n",
    "    userDict[\"2nd\"] = decileProjects[1]\n",
    "    userDict[\"3rd\"] = decileProjects[2]\n",
    "    userDict[\"4th\"] = decileProjects[3]\n",
    "    userDict[\"5th\"] = decileProjects[4]\n",
    "    userDict[\"6th\"] = decileProjects[5]\n",
    "    userDict[\"7th\"] = decileProjects[6]\n",
    "    userDict[\"8th\"] = decileProjects[7]\n",
    "    userDict[\"9th\"] = decileProjects[8]\n",
    "    userDict[\"10th\"] = decileProjects[9]\n",
    "\n",
    "    return userDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dayAnalysisFeatures(projects): \n",
    "    userDict = {} \n",
    "    userDict[\"weekday\"] = percentOnWeekday(projects) #\n",
    "    day = percentOnDay(projects)\n",
    "    userDict[\"Monday\"] = day[0]\n",
    "    userDict[\"Tuesday\"] = day[1]\n",
    "    userDict[\"Wednesday\"] =day[2]\n",
    "    userDict[\"Thursday\"] = day[3]\n",
    "    userDict[\"Friday\"] = day[4]\n",
    "    userDict[\"Saturday\"] = day[5]\n",
    "    userDict[\"Sunday\"] = day[6]\n",
    "\n",
    "    return userDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summaryOBlockDecile(projects): \n",
    "    userDict = {}\n",
    "    decileOrphanBlock = decileOrphanBlocks(projects)\n",
    "    userDict[\"O 1\"] = decileOrphanBlock[0]\n",
    "    userDict[\"O 2\"] = decileOrphanBlock[1]\n",
    "    userDict[\"O 3\"] = decileOrphanBlock[2]\n",
    "    userDict[\"O 4\"] = decileOrphanBlock[3]\n",
    "    userDict[\"O 5\"] = decileOrphanBlock[4]\n",
    "    userDict[\"O 6\"] = decileOrphanBlock[5]\n",
    "    userDict[\"O 7\"] = decileOrphanBlock[6]\n",
    "    userDict[\"O 8\"] = decileOrphanBlock[7]\n",
    "    userDict[\"O 9\"] = decileOrphanBlock[8]\n",
    "    userDict[\"O 10\"] = decileOrphanBlock[9]\n",
    "    return userDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summaryDecileTLBlocks(projects): \n",
    "    userDict = {}\n",
    "    decileNumTopLevelBlock = decileNumTopLevelBlocks(projects)\n",
    "    userDict[\"TL 1\"] = decileNumTopLevelBlock[0]\n",
    "    userDict[\"TL 2\"] = decileNumTopLevelBlock[1]\n",
    "    userDict[\"TL 3\"] = decileNumTopLevelBlock[2]\n",
    "    userDict[\"TL 4\"] = decileNumTopLevelBlock[3]\n",
    "    userDict[\"TL 5\"] = decileNumTopLevelBlock[4]\n",
    "    userDict[\"TL 6\"] = decileNumTopLevelBlock[5]\n",
    "    userDict[\"TL 7\"] = decileNumTopLevelBlock[6]\n",
    "    userDict[\"TL 8\"] = decileNumTopLevelBlock[7]\n",
    "    userDict[\"TL 9\"] = decileNumTopLevelBlock[8]\n",
    "    userDict[\"TL 10\"] = decileNumTopLevelBlock[9]\n",
    "    return userDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summaryDecileNumScreens(projects): \n",
    "    userDict = {}\n",
    "    decileNumS = decileNumScreens(projects)\n",
    "    userDict[\"NS 1\"] = decileNumS[0]\n",
    "    userDict[\"NS 2\"] = decileNumS[1]\n",
    "    userDict[\"NS 3\"] = decileNumS[2]\n",
    "    userDict[\"NS 4\"] = decileNumS[3]\n",
    "    userDict[\"NS 5\"] = decileNumS[4]\n",
    "    userDict[\"NS 6\"] = decileNumS[5]\n",
    "    userDict[\"NS 7\"] = decileNumS[6]\n",
    "    userDict[\"NS 8\"] = decileNumS[7]\n",
    "    userDict[\"NS 9\"] = decileNumS[8]\n",
    "    userDict[\"NS 10\"] = decileNumS[9]\n",
    "    return userDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summaryAverages(projects):\n",
    "    userDict = {} \n",
    "    userDict[\"NS\"] = averageNumScreens(projects)\n",
    "    userDict[\"NB\"] = averageNumBlocks(projects)\n",
    "    userDict[\"OB\"] = getAverageOrphanBlocks(projects)\n",
    "    \n",
    "    varList = getAllVariables(projects)\n",
    "    \n",
    "    userDict[\"local vars\"] = varList[0]\n",
    "    userDict[\"global vars\"] = varList[1]\n",
    "    \n",
    "    return userDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write other feature functions -- perhaps different interval sizes (instead of 10%), proportion of projects rather than number, etc. With some effort, we can get a measure of activity on weekends/evenings as features too. Experiment...\n",
    "\n",
    "Give each feature function a distinct name, and re-run the above two cells with the function in place of `featurize` to see if the contingency tables change\n",
    "\n",
    "Remember from the CS111 assignment that we can combine a list of feature functions to give a new feature function, so you should experiment with combinations as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_featfuncs(funclist):\n",
    "    def combined(user):\n",
    "        basedict = funclist[0](user)\n",
    "        for f in funclist[1:]:\n",
    "            basedict.update(f(user))\n",
    "        return basedict\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RETENTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "user_dicts = []\n",
    "# i = 0\n",
    "\n",
    "if TASK == \"RETENTION\":\n",
    "    i = 0\n",
    "    for user in OD:\n",
    "        i +=1 \n",
    "        if i % 1000 == 0: \n",
    "            print i\n",
    "        projects = getProjects(user)\n",
    "        all_combined = combine_featfuncs([percentProjectsFeatures, dayAnalysisFeatures, summaryDecileNumScreens,summaryOBlockDecile,summaryDecileTLBlocks,summaryAverages])\n",
    "        time_combined = combine_featfuncs([percentProjectsFeatures, dayAnalysisFeatures])\n",
    "        code_combined = combine_featfuncs([summaryDecileNumScreens,summaryOBlockDecile,summaryDecileTLBlocks,summaryAverages]) \n",
    "    \n",
    "        all_features.append(all_combined(projects))\n",
    "        time_features.append(time_combined(projects))\n",
    "        code_features.append(code_combined(projects))\n",
    "\n",
    "\n",
    "    print ('done loading featurizers')\n",
    "    \n",
    "\n",
    "    vec = DictVectorizer()\n",
    "    X1 = vec.fit_transform(all_features) # converts to array: http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    X2 = vec.fit_transform(time_features)\n",
    "    X3 = vec.fit_transform(code_features)\n",
    "    #X = vec.fit_transform(user_dicts) # converts to array: http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "    #X = normalize(X, axis=0, norm='l1') #normalization is bringing the values down significantly \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 1: Clustering.** Is there any correlation between the clusters produced by these features, and whether or not a user is retained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 25324 is out of bounds for axis 0 with size 25324",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-2d4ee4b83501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers_clusterids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musers_clusterids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mold_user_activity_length\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mMINDUR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mconting\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 25324 is out of bounds for axis 0 with size 25324"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "if TASK == 'RETENTION':\n",
    "\n",
    "    KMeans_model = KMeans(n_clusters=2)\n",
    "    users_clusterids = KMeans_model.fit_predict(X1)\n",
    "\n",
    "    from scipy.stats import chi2_contingency\n",
    "    conting = numpy.zeros((2, 2))  # row = cluster ID (0, 1), col = #days (0 for <=120, 1 for >120)\n",
    "\n",
    "    for i in range(len(users_clusterids)):\n",
    "        cluster = users_clusterids[i]\n",
    "        if old_user_activity_length[i]<=MINDUR:\n",
    "            conting[cluster, 0] += 1\n",
    "        else:\n",
    "            conting[cluster, 1] += 1\n",
    "\n",
    "# visualize\n",
    "    print conting\n",
    "\n",
    "# compute chi2 statistic to get the probability that there is no association between\n",
    "# the clustering and whether or not the user is active for >120 days\n",
    "# lower probabilities are better\n",
    "    _, p, _, _ = chi2_contingency(conting)\n",
    "    print 'There is a', p, 'probability that the clusters and retention are independent.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 2: Classification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chance is 0.52772073922\n",
      "Building a model with 55 features for RETENTION\n",
      "Fold\n",
      "70 neighbors\n",
      "All features KNN Score:  0.515280739161\n",
      "All features KNN f1 score 0.36247467664 \n",
      "\n",
      "All features Logistic Regression Score:  0.513740819711\n",
      "All features Logistic f1 score 0.217051306504\n",
      "Time features KNN Score:  0.507936507937\n",
      "Time features KNN f1 score 0.342305256491 \n",
      "\n",
      "Time features Logistic Regression Score:  0.526533996683\n",
      "Time features Logistic f1 score 0.12327264751\n",
      "Code features KNN Score:  0.507936507937\n",
      "Code features KNN f1 score 0.342305256491 \n",
      "\n",
      "Code features Logistic Regression Score:  0.520374318882\n",
      "Code features Logistic f1 score 0.165670719143\n",
      "Fold\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-2f73d036cbfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mlogistic_model3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX3train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mKNN_XtestP3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighbors_model3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX3test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mlog_XtestP3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_model3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX3test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/neighbors/classification.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/neighbors/base.pyc\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 dist = pairwise_distances(X, self._fit_X, 'euclidean',\n\u001b[0;32m--> 371\u001b[0;31m                                           n_jobs=n_jobs, squared=True)\n\u001b[0m\u001b[1;32m    372\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 dist = pairwise_distances(\n",
      "\u001b[0;32m/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1205\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;31m# Special case to avoid picklability checks in delayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[0;31m# TODO: in some cases, backend='threading' may be appropriate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mYY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/utils/extmath.pyc\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \"\"\"\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    504\u001b[0m            \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m            \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m            indptr, indices, data)\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from  sklearn.metrics import f1_score \n",
    "from sklearn import linear_model\n",
    "\n",
    "if TASK=='RETENTION':\n",
    "    y = numpy.array(map(lambda d: int(d>MINDUR), old_user_activity_length))  # labels (0 for less than MINDUR, 1 otherwise)\n",
    "\n",
    "    percentpos = sum(y)/float(len(y))\n",
    "    chance = max(percentpos, 1-percentpos)\n",
    "    print 'Chance is', chance\n",
    "\n",
    "    print 'Building a model with', X1.shape[1], 'features for', TASK\n",
    "    all_coefs = [] \n",
    "    for trainidx, testidx in StratifiedKFold(y):\n",
    "        print 'Fold'\n",
    "        k = 70 #num neighbors\n",
    "        ytrain = y[trainidx]\n",
    "        ytest = y[testidx]\n",
    "        \n",
    "        X1train = X1[trainidx, :]  # using numpy's smart indexing\n",
    "        X1test = X1[testidx, :]\n",
    "        \n",
    "        X2train = X2[trainidx, :]  # using numpy's smart indexing\n",
    "        X2test = X2[testidx, :]\n",
    "    \n",
    "        X3train = X3[trainidx, :]  # using numpy's smart indexing\n",
    "        X3test = X3[testidx, :]\n",
    "    \n",
    "        KNeighbors_model1 = KNeighborsClassifier(n_neighbors=k)\n",
    "        logistic_model1 = linear_model.LogisticRegression(fit_intercept= False)\n",
    "\n",
    "        KNeighbors_model2 = KNeighborsClassifier(n_neighbors=k)\n",
    "        logistic_model2 = linear_model.LogisticRegression(fit_intercept= False)\n",
    "\n",
    "        KNeighbors_model3 = KNeighborsClassifier(n_neighbors=k)  \n",
    "        logistic_model3 = linear_model.LogisticRegression(fit_intercept= False)\n",
    "\n",
    "        KNeighbors_model1.fit(X1train, ytrain)\n",
    "        logistic_model1.fit(X1train, ytrain)\n",
    "\n",
    "        KNN_XtestP1 = KNeighbors_model1.predict(X1test)\n",
    "        log_XtestP1 = logistic_model1.predict(X1test)\n",
    "    \n",
    "        KNeighbors_model2.fit(X2train, ytrain)\n",
    "        logistic_model2.fit(X2train, ytrain)\n",
    "\n",
    "        KNN_XtestP2 = KNeighbors_model2.predict(X2test)\n",
    "        log_XtestP2 = logistic_model2.predict(X2test)\n",
    "        \n",
    "        KNeighbors_model3.fit(X3train, ytrain)\n",
    "        logistic_model3.fit(X3train, ytrain)\n",
    "\n",
    "        KNN_XtestP3 = KNeighbors_model3.predict(X3test)\n",
    "        log_XtestP3 = logistic_model3.predict(X3test)\n",
    "    \n",
    "    \n",
    "    \n",
    "        print k, 'neighbors' \n",
    "        print \"All features KNN Score: \", KNeighbors_model1.score(X1test, ytest)\n",
    "        print 'All features KNN f1 score', f1_score(KNN_XtestP1, ytest),'\\n'\n",
    "\n",
    "        print'All features Logistic Regression Score: ', logistic_model1.score(X1test, ytest)\n",
    "        print 'All features Logistic f1 score', f1_score(log_XtestP1, ytest) \n",
    "\n",
    "        print \"Time features KNN Score: \", KNeighbors_model2.score(X2test, ytest)\n",
    "        print 'Time features KNN f1 score', f1_score(KNN_XtestP2, ytest),'\\n'\n",
    "\n",
    "        print'Time features Logistic Regression Score: ', logistic_model2.score(X2test, ytest)\n",
    "        print 'Time features Logistic f1 score', f1_score(log_XtestP2, ytest) \n",
    "        \n",
    "        print \"Code features KNN Score: \", KNeighbors_model2.score(X2test, ytest)\n",
    "        print 'Code features KNN f1 score', f1_score(KNN_XtestP2, ytest),'\\n'\n",
    "\n",
    "        print'Code features Logistic Regression Score: ', logistic_model3.score(X3test, ytest)\n",
    "        print 'Code features Logistic f1 score', f1_score(log_XtestP3, ytest) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "LANGUAGE:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading keys\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "done loading featurizers\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b7ee90edb228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#X = normalize(X, axis=0, norm='l1') #normalization is bringing the values down significantly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "all_features = []\n",
    "time_features = [] \n",
    "code_features = []\n",
    "\n",
    "users = lang_data.keys()\n",
    "print ('done loading keys')\n",
    "i = 0 \n",
    "for user in users:\n",
    "    i +=1 \n",
    "    if i % 1000 == 0: \n",
    "        print i\n",
    "    projects = getProjects(user)\n",
    "    all_combined = combine_featfuncs([percentProjectsFeatures, dayAnalysisFeatures, summaryDecileNumScreens,summaryOBlockDecile,summaryDecileTLBlocks,summaryAverages])\n",
    "    time_combined = combine_featfuncs([percentProjectsFeatures, dayAnalysisFeatures])\n",
    "    code_combined = combine_featfuncs([summaryDecileNumScreens,summaryOBlockDecile,summaryDecileTLBlocks,summaryAverages]) \n",
    "\n",
    "    all_features.append(all_combined(projects))\n",
    "    time_features.append(time_combined(projects))\n",
    "    code_features.append(code_combined(projects))\n",
    "    \n",
    "# for userID in lang_data: #user is just the user's id num \n",
    "#     combined = combine_featfuncs([percentProjectsFeatures, dayAnalysisFeatures, decileFeatures ,summaryNumScreensDecile,summaryOBlockDecile,summaryDecileTLBlocks,summaryAverages]) #a function object is being returned here?? \n",
    "#     user_dicts.append(combined(userID))\n",
    "\n",
    "\n",
    "print ('done loading featurizers')\n",
    "    \n",
    "\n",
    "vec = DictVectorizer()\n",
    "X1 = vec.fit_transform(all_features) # converts to array: http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "X2 = vec.fit_transform(time_features)\n",
    "X3 = vec.fit_transform(code_features)\n",
    "#X = normalize(X, axis=0, norm='l1') #normalization is bringing the values down significantly \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print len(data)\n",
    "# print len(lang_data)\n",
    "# #what y is\n",
    "# print [lang_dict[user_langs[userID]] for userID in lang_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 209.  237.  238.  416.  238.  496.  131.  115.  386.   77.  474.   73.\n",
      "  393.  357.  352.  259.  288.   80.  671.  523.   21.  448.  399.  110.\n",
      "  111.   87.   82.  205.  176.  296.  808.  539.  403.   87.  274.  118.\n",
      "  275.  734.  706.  201.  276.  315.  238.  361.  394.  320.   77.  116.\n",
      "   40.   37.  525.   84.  739.   25.   59.  119.   91.  766.  103.  232.\n",
      "  107.   44.  298.   13.   83.  141.  436.  270.  471.  111.  739.   87.\n",
      "   55.  477.  270.  209.   63.   13.   49.  104.  424.  516.  216.  228.\n",
      "  269.   98.  119.   84.  111.  283.  736.  268.  114.   86.  678.  111.\n",
      "  122.  130.  228.   97.]\n",
      "Chance is 44.59384683\n",
      "Fold\n",
      "models created\n",
      "models fit\n",
      "models predicted \n",
      "\n",
      "70 neighbors\n",
      "All features KNN Score:  0.592261121348\n",
      "All features KNN f1 score"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1076: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.681591036075 \n",
      "\n",
      "time features KNN Score:  0.589892076862\n",
      "time features KNN f1 score 0.680842365005 \n",
      "\n",
      "code features KNN Score: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.504211634641\n",
      "code features KNN f1 score 0.593063706722 \n",
      "\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold\n",
      "models created\n",
      "models fit\n",
      "models predicted \n",
      "\n",
      "70 neighbors\n",
      "All features KNN Score:  0.590186638528\n",
      "All features KNN f1 score 0.675606318659 \n",
      "\n",
      "time features KNN Score: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.587482688122\n",
      "time features KNN f1 score 0.6756788812 \n",
      "\n",
      "code features KNN Score: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.506957726044\n",
      "code features KNN f1 score 0.598597681528 \n",
      "\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "Fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models created\n",
      "models fit\n",
      "models predicted \n",
      "\n",
      "70 neighbors\n",
      "All features KNN Score:  0.594810511026\n",
      "All features KNN f1 score 0.681477205817 \n",
      "\n",
      "time features KNN Score: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.591509309389\n",
      "time features KNN f1 score 0.680737852999 \n",
      "\n",
      "code features KNN Score: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.508253004093\n",
      "code features KNN f1 score 0.596349186174 \n",
      "\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmalurie/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from  sklearn.metrics import f1_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "print old_user_activity_length[0:100]\n",
    "if TASK=='LANGUAGE':\n",
    "    y =  numpy.array([lang_dict[user_langs[userID]] for userID in lang_data]) # labels\n",
    "\n",
    "percentpos = sum(y)/float(len(y))\n",
    "chance = max(percentpos, 1-percentpos)\n",
    "print 'Chance is', chance\n",
    "\n",
    "#print 'Building a model with', X1.shape[1], 'features for', TASK\n",
    "all_coefs = [] \n",
    "for trainidx, testidx in StratifiedKFold(y):\n",
    "    print 'Fold'\n",
    "    k = 70 #num neighbors\n",
    "    X1train = X1[trainidx, :]  # using numpy's smart indexing\n",
    "    X1test = X1[testidx, :]\n",
    "    ytrain = y[trainidx]\n",
    "    ytest = y[testidx]\n",
    "    \n",
    "    X2train = X2[trainidx, :]  # using numpy's smart indexing\n",
    "    X2test = X2[testidx, :]\n",
    "    \n",
    "    X3train = X3[trainidx, :]  # using numpy's smart indexing\n",
    "    X3test = X3[testidx, :]\n",
    "    \n",
    "    KNeighbors_model1 = KNeighborsClassifier(n_neighbors=k)  \n",
    "    KNeighbors_model2 = KNeighborsClassifier(n_neighbors=k)  \n",
    "    KNeighbors_model3 = KNeighborsClassifier(n_neighbors=k)  \n",
    "    \n",
    "    print('models created')\n",
    "\n",
    "    KNeighbors_model1.fit(X1train, ytrain)\n",
    "    print 'model1 fit'\n",
    "    KNeighbors_model2.fit(X2train, ytrain)\n",
    "    print 'model2 fit'\n",
    "    KNeighbors_model3.fit(X3train, ytrain)  \n",
    "    print ('all models fit')\n",
    "    \n",
    "    KNN_XtestP1 = KNeighbors_model1.predict(X1test)\n",
    "    print 'model1 predicted'\n",
    "    KNN_XtestP2 = KNeighbors_model2.predict(X2test)\n",
    "    print 'model2 predicted'\n",
    "    KNN_XtestP3 = KNeighbors_model3.predict(X3test)\n",
    "    print 'models predicted \\n'\n",
    "    \n",
    "    print k, 'neighbors' \n",
    "    print \"All features KNN Score: \", KNeighbors_model1.score(X1test, ytest)\n",
    "    print 'All features KNN f1 score', f1_score(KNN_XtestP1, ytest),'\\n'\n",
    "\n",
    "    print \"time features KNN Score: \", KNeighbors_model2.score(X2test, ytest)\n",
    "    print 'time features KNN f1 score', f1_score(KNN_XtestP2, ytest),'\\n'\n",
    "\n",
    "    print \"code features KNN Score: \", KNeighbors_model3.score(X3test, ytest)\n",
    "    print 'code features KNN f1 score', f1_score(KNN_XtestP3, ytest),'\\n'\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(ytest, KNN_XtestP1)\n",
    "    print cnf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [ipykernel_py2]",
   "language": "python",
   "name": "Python [ipykernel_py2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
